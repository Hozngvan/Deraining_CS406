{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004ba4ab",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-21T06:02:51.046155Z",
     "iopub.status.busy": "2025-12-21T06:02:51.045838Z",
     "iopub.status.idle": "2025-12-21T06:02:51.498434Z",
     "shell.execute_reply": "2025-12-21T06:02:51.497637Z"
    },
    "papermill": {
     "duration": 0.457585,
     "end_time": "2025-12-21T06:02:51.499810",
     "exception": false,
     "start_time": "2025-12-21T06:02:51.042225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/mprnetv2/MPRNet /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb936230",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:02:51.505252Z",
     "iopub.status.busy": "2025-12-21T06:02:51.505013Z",
     "iopub.status.idle": "2025-12-21T06:02:51.513838Z",
     "shell.execute_reply": "2025-12-21T06:02:51.513165Z"
    },
    "papermill": {
     "duration": 0.012785,
     "end_time": "2025-12-21T06:02:51.514948",
     "exception": false,
     "start_time": "2025-12-21T06:02:51.502163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/MPRNet/Deraining\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/MPRNet/Deraining/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1aed355",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:02:51.520502Z",
     "iopub.status.busy": "2025-12-21T06:02:51.520013Z",
     "iopub.status.idle": "2025-12-21T06:02:58.075395Z",
     "shell.execute_reply": "2025-12-21T06:02:58.074650Z"
    },
    "papermill": {
     "duration": 6.55962,
     "end_time": "2025-12-21T06:02:58.076698",
     "exception": false,
     "start_time": "2025-12-21T06:02:51.517078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yacs\r\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\r\n",
      "Collecting warmup_scheduler\r\n",
      "  Downloading warmup_scheduler-0.3.tar.gz (2.1 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from yacs) (6.0.3)\r\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\r\n",
      "Building wheels for collected packages: warmup_scheduler\r\n",
      "  Building wheel for warmup_scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for warmup_scheduler: filename=warmup_scheduler-0.3-py3-none-any.whl size=2971 sha256=ec6ace5a61ac6a3d482b387318c15abf32f1aaa45071c62c98c254d3f940daf0\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/cc/5c/3b/6e5033100e0e4191383dad5c4279638a37f9791d1af9e1d85c\r\n",
      "Successfully built warmup_scheduler\r\n",
      "Installing collected packages: warmup_scheduler, yacs\r\n",
      "Successfully installed warmup_scheduler-0.3 yacs-0.1.8\r\n"
     ]
    }
   ],
   "source": [
    "!pip install yacs warmup_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32bd68a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:02:58.083048Z",
     "iopub.status.busy": "2025-12-21T06:02:58.082784Z",
     "iopub.status.idle": "2025-12-21T06:02:58.199939Z",
     "shell.execute_reply": "2025-12-21T06:02:58.199023Z"
    },
    "papermill": {
     "duration": 0.121765,
     "end_time": "2025-12-21T06:02:58.201232",
     "exception": false,
     "start_time": "2025-12-21T06:02:58.079467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p /kaggle/working/MPRNet/Deraining/checkpoints/Deraining/models/MPRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dcb0db6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:02:58.207296Z",
     "iopub.status.busy": "2025-12-21T06:02:58.206672Z",
     "iopub.status.idle": "2025-12-21T06:02:58.917341Z",
     "shell.execute_reply": "2025-12-21T06:02:58.916511Z"
    },
    "papermill": {
     "duration": 0.715245,
     "end_time": "2025-12-21T06:02:58.918768",
     "exception": false,
     "start_time": "2025-12-21T06:02:58.203523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/mprnet-checkpoints-rain14000/model_epoch_20.pth /kaggle/working/MPRNet/Deraining/checkpoints/Deraining/models/MPRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f565fc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:02:58.925204Z",
     "iopub.status.busy": "2025-12-21T06:02:58.924953Z",
     "iopub.status.idle": "2025-12-21T06:02:58.952073Z",
     "shell.execute_reply": "2025-12-21T06:02:58.951417Z"
    },
    "papermill": {
     "duration": 0.031462,
     "end_time": "2025-12-21T06:02:58.953168",
     "exception": false,
     "start_time": "2025-12-21T06:02:58.921706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated training.yml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "yaml_path = '/kaggle/working/MPRNet/Deraining/training.yml'\n",
    "\n",
    "# load file\n",
    "with open(yaml_path, 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# chỉnh sửa config\n",
    "cfg['GPU'] = [0]\n",
    "cfg['OPTIM']['BATCH_SIZE'] = 4\n",
    "cfg['OPTIM']['NUM_EPOCHS'] = 100\n",
    "cfg['OPTIM']['LR_INITIAL'] = 2e-4\n",
    "cfg['OPTIM']['LR_MIN'] = 1e-6\n",
    "\n",
    "cfg['TRAINING']['TRAIN_DIR'] = '/kaggle/input/rain14000/train'\n",
    "cfg['TRAINING']['VAL_DIR'] = '/kaggle/input/rain14000/val'\n",
    "cfg['TRAINING']['SAVE_DIR'] = '/kaggle/working/checkpoints'\n",
    "\n",
    "cfg['TRAINING']['RESUME'] = True\n",
    "\n",
    "# save lại file\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(cfg, f)\n",
    "\n",
    "print(\"Updated training.yml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc3c9916",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:02:58.959098Z",
     "iopub.status.busy": "2025-12-21T06:02:58.958884Z",
     "iopub.status.idle": "2025-12-21T06:02:58.965354Z",
     "shell.execute_reply": "2025-12-21T06:02:58.964762Z"
    },
    "papermill": {
     "duration": 0.010897,
     "end_time": "2025-12-21T06:02:58.966494",
     "exception": false,
     "start_time": "2025-12-21T06:02:58.955597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset_RGB.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset_RGB.py\n",
    "# ghi đè toàn bộ nội dung mới\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "from pdb import set_trace as stx\n",
    "import random\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in ['jpeg', 'JPEG', 'jpg', 'png', 'JPG', 'PNG', 'gif'])\n",
    "\n",
    "class DataLoaderTrain(Dataset):\n",
    "    def __init__(self, rgb_dir, img_options=None):\n",
    "        super(DataLoaderTrain, self).__init__()\n",
    "\n",
    "        inp_files = sorted(os.listdir(os.path.join(rgb_dir, 'input')))\n",
    "        tar_files = sorted(os.listdir(os.path.join(rgb_dir, 'output')))\n",
    "\n",
    "        self.inp_filenames = [os.path.join(rgb_dir, 'input', x)  for x in inp_files if is_image_file(x)]\n",
    "        self.tar_filenames = [os.path.join(rgb_dir, 'output', x) for x in tar_files if is_image_file(x)]\n",
    "\n",
    "        self.img_options = img_options\n",
    "        self.sizex       = len(self.tar_filenames)  # get the size of target\n",
    "\n",
    "        self.ps = self.img_options['patch_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sizex\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_ = index % self.sizex\n",
    "        ps = self.ps\n",
    "\n",
    "        inp_path = self.inp_filenames[index_]\n",
    "        tar_path = self.tar_filenames[index_]\n",
    "\n",
    "        inp_img = Image.open(inp_path)\n",
    "        tar_img = Image.open(tar_path)\n",
    "\n",
    "        w,h = tar_img.size\n",
    "        padw = ps-w if w<ps else 0\n",
    "        padh = ps-h if h<ps else 0\n",
    "\n",
    "        # Reflect Pad in case image is smaller than patch_size\n",
    "        if padw!=0 or padh!=0:\n",
    "            inp_img = TF.pad(inp_img, (0,0,padw,padh), padding_mode='reflect')\n",
    "            tar_img = TF.pad(tar_img, (0,0,padw,padh), padding_mode='reflect')\n",
    "\n",
    "        inp_img = TF.to_tensor(inp_img)\n",
    "        tar_img = TF.to_tensor(tar_img)\n",
    "\n",
    "        hh, ww = tar_img.shape[1], tar_img.shape[2]\n",
    "\n",
    "        rr     = random.randint(0, hh-ps)\n",
    "        cc     = random.randint(0, ww-ps)\n",
    "        aug    = random.randint(0, 8)\n",
    "\n",
    "        # Crop patch\n",
    "        inp_img = inp_img[:, rr:rr+ps, cc:cc+ps]\n",
    "        tar_img = tar_img[:, rr:rr+ps, cc:cc+ps]\n",
    "\n",
    "        # Data Augmentations\n",
    "        if aug==1:\n",
    "            inp_img = inp_img.flip(1)\n",
    "            tar_img = tar_img.flip(1)\n",
    "        elif aug==2:\n",
    "            inp_img = inp_img.flip(2)\n",
    "            tar_img = tar_img.flip(2)\n",
    "        elif aug==3:\n",
    "            inp_img = torch.rot90(inp_img,dims=(1,2))\n",
    "            tar_img = torch.rot90(tar_img,dims=(1,2))\n",
    "        elif aug==4:\n",
    "            inp_img = torch.rot90(inp_img,dims=(1,2), k=2)\n",
    "            tar_img = torch.rot90(tar_img,dims=(1,2), k=2)\n",
    "        elif aug==5:\n",
    "            inp_img = torch.rot90(inp_img,dims=(1,2), k=3)\n",
    "            tar_img = torch.rot90(tar_img,dims=(1,2), k=3)\n",
    "        elif aug==6:\n",
    "            inp_img = torch.rot90(inp_img.flip(1),dims=(1,2))\n",
    "            tar_img = torch.rot90(tar_img.flip(1),dims=(1,2))\n",
    "        elif aug==7:\n",
    "            inp_img = torch.rot90(inp_img.flip(2),dims=(1,2))\n",
    "            tar_img = torch.rot90(tar_img.flip(2),dims=(1,2))\n",
    "        \n",
    "        filename = os.path.splitext(os.path.split(tar_path)[-1])[0]\n",
    "\n",
    "        return tar_img, inp_img, filename\n",
    "\n",
    "class DataLoaderVal(Dataset):\n",
    "    def __init__(self, rgb_dir, img_options=None, rgb_dir2=None):\n",
    "        super(DataLoaderVal, self).__init__()\n",
    "\n",
    "        inp_files = sorted(os.listdir(os.path.join(rgb_dir, 'input')))\n",
    "        tar_files = sorted(os.listdir(os.path.join(rgb_dir, 'output')))\n",
    "\n",
    "        self.inp_filenames = [os.path.join(rgb_dir, 'input', x)  for x in inp_files if is_image_file(x)]\n",
    "        self.tar_filenames = [os.path.join(rgb_dir, 'output', x) for x in tar_files if is_image_file(x)]\n",
    "\n",
    "        self.img_options = img_options\n",
    "        self.sizex       = len(self.tar_filenames)  # get the size of target\n",
    "\n",
    "        self.ps = self.img_options['patch_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sizex\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_ = index % self.sizex\n",
    "        ps = self.ps\n",
    "\n",
    "        inp_path = self.inp_filenames[index_]\n",
    "        tar_path = self.tar_filenames[index_]\n",
    "\n",
    "        inp_img = Image.open(inp_path)\n",
    "        tar_img = Image.open(tar_path)\n",
    "\n",
    "        # Validate on center crop\n",
    "        if self.ps is not None:\n",
    "            inp_img = TF.center_crop(inp_img, (ps,ps))\n",
    "            tar_img = TF.center_crop(tar_img, (ps,ps))\n",
    "\n",
    "        inp_img = TF.to_tensor(inp_img)\n",
    "        tar_img = TF.to_tensor(tar_img)\n",
    "\n",
    "        filename = os.path.splitext(os.path.split(tar_path)[-1])[0]\n",
    "\n",
    "        return tar_img, inp_img, filename\n",
    "\n",
    "class DataLoaderTest(Dataset):\n",
    "    def __init__(self, inp_dir, img_options):\n",
    "        super(DataLoaderTest, self).__init__()\n",
    "\n",
    "        inp_files = sorted(os.listdir(inp_dir))\n",
    "        self.inp_filenames = [os.path.join(inp_dir, x) for x in inp_files if is_image_file(x)]\n",
    "\n",
    "        self.inp_size = len(self.inp_filenames)\n",
    "        self.img_options = img_options\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inp_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path_inp = self.inp_filenames[index]\n",
    "        filename = os.path.splitext(os.path.split(path_inp)[-1])[0]\n",
    "        inp = Image.open(path_inp)\n",
    "\n",
    "        inp = TF.to_tensor(inp)\n",
    "        return inp, filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f57ac64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:02:58.972348Z",
     "iopub.status.busy": "2025-12-21T06:02:58.972169Z",
     "iopub.status.idle": "2025-12-21T06:02:58.978867Z",
     "shell.execute_reply": "2025-12-21T06:02:58.978296Z"
    },
    "papermill": {
     "duration": 0.011078,
     "end_time": "2025-12-21T06:02:58.979951",
     "exception": false,
     "start_time": "2025-12-21T06:02:58.968873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import os\n",
    "from config import Config \n",
    "opt = Config('training.yml')\n",
    "\n",
    "gpus = ','.join([str(i) for i in opt.GPU])\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpus\n",
    "\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "from data_RGB import get_training_data, get_validation_data\n",
    "from MPRNet import MPRNet\n",
    "import losses\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from tqdm import tqdm\n",
    "from pdb import set_trace as stx\n",
    "\n",
    "######### Set Seeds ###########\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed_all(1234)\n",
    "\n",
    "start_epoch = 1\n",
    "mode = opt.MODEL.MODE\n",
    "session = opt.MODEL.SESSION\n",
    "\n",
    "result_dir = os.path.join(opt.TRAINING.SAVE_DIR, mode, 'results', session)\n",
    "model_dir  = os.path.join(opt.TRAINING.SAVE_DIR, mode, 'models',  session)\n",
    "\n",
    "utils.mkdir(result_dir)\n",
    "utils.mkdir(model_dir)\n",
    "\n",
    "train_dir = opt.TRAINING.TRAIN_DIR\n",
    "val_dir   = opt.TRAINING.VAL_DIR\n",
    "\n",
    "######### Model ###########\n",
    "model_restoration = MPRNet()\n",
    "model_restoration.cuda()\n",
    "\n",
    "device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"\\n\\nLet's use\", torch.cuda.device_count(), \"GPUs!\\n\\n\")\n",
    "\n",
    "\n",
    "new_lr = opt.OPTIM.LR_INITIAL\n",
    "\n",
    "optimizer = optim.Adam(model_restoration.parameters(), lr=new_lr, betas=(0.9, 0.999),eps=1e-8)\n",
    "\n",
    "\n",
    "######### Scheduler ###########\n",
    "warmup_epochs = 3\n",
    "scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, opt.OPTIM.NUM_EPOCHS-warmup_epochs, eta_min=opt.OPTIM.LR_MIN)\n",
    "scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=warmup_epochs, after_scheduler=scheduler_cosine)\n",
    "# scheduler.step()\n",
    "\n",
    "######### Resume ###########\n",
    "# if opt.TRAINING.RESUME:\n",
    "#     path_chk_rest    = utils.get_last_path(model_dir, 'model_epoch_*.pth')\n",
    "#     utils.load_checkpoint(model_restoration,path_chk_rest)\n",
    "#     start_epoch = utils.load_start_epoch(path_chk_rest) + 1\n",
    "#     utils.load_optim(optimizer, path_chk_rest)\n",
    "\n",
    "#     for i in range(1, start_epoch):\n",
    "#         scheduler.step()\n",
    "#     new_lr = scheduler.get_lr()[0]\n",
    "#     print('------------------------------------------------------------------------------')\n",
    "#     print(\"==> Resuming Training with learning rate:\", new_lr)\n",
    "#     print('------------------------------------------------------------------------------')\n",
    "\n",
    "if opt.TRAINING.RESUME:\n",
    "    path_chk_rest = '/kaggle/input/mprnet-checkpoints-rain14000/model_epoch_80.pth'\n",
    "\n",
    "    if path_chk_rest is not None:\n",
    "        utils.load_checkpoint(model_restoration, path_chk_rest)\n",
    "        start_epoch = utils.load_start_epoch(path_chk_rest) + 1\n",
    "        utils.load_optim(optimizer, path_chk_rest)\n",
    "\n",
    "        # step scheduler đúng số epoch đã train\n",
    "        for _ in range(1, start_epoch):\n",
    "            scheduler.step()\n",
    "\n",
    "        new_lr = scheduler.get_last_lr()[0]\n",
    "        print('------------------------------------------------------------------------------')\n",
    "        print(\"==> Resuming Training from epoch {} with LR {:.8f}\".format(start_epoch, new_lr))\n",
    "        print('------------------------------------------------------------------------------')\n",
    "    else:\n",
    "        print(\"⚠️ No checkpoint found. Training from scratch.\")\n",
    "        start_epoch = 1\n",
    "\n",
    "if len(device_ids)>1:\n",
    "    model_restoration = nn.DataParallel(model_restoration, device_ids = device_ids)\n",
    "\n",
    "######### Loss ###########\n",
    "criterion_char = losses.CharbonnierLoss()\n",
    "criterion_edge = losses.EdgeLoss()\n",
    "\n",
    "######### DataLoaders ###########\n",
    "train_dataset = get_training_data(train_dir, {'patch_size':opt.TRAINING.TRAIN_PS})\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=opt.OPTIM.BATCH_SIZE, shuffle=True, num_workers=16, drop_last=False, pin_memory=True)\n",
    "\n",
    "val_dataset = get_validation_data(val_dir, {'patch_size':opt.TRAINING.VAL_PS})\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=16, shuffle=False, num_workers=8, drop_last=False, pin_memory=True)\n",
    "\n",
    "print('===> Start Epoch {} End Epoch {}'.format(start_epoch,opt.OPTIM.NUM_EPOCHS + 1))\n",
    "print('===> Loading datasets')\n",
    "\n",
    "best_psnr = 31.0807\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, opt.OPTIM.NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    epoch_loss = 0\n",
    "    train_id = 1\n",
    "\n",
    "    model_restoration.train()\n",
    "    for i, data in enumerate(tqdm(train_loader), 0):\n",
    "\n",
    "        # zero_grad\n",
    "        for param in model_restoration.parameters():\n",
    "            param.grad = None\n",
    "\n",
    "        target = data[0].cuda()\n",
    "        input_ = data[1].cuda()\n",
    "\n",
    "        restored = model_restoration(input_)\n",
    " \n",
    "        # Compute loss at each stage\n",
    "        loss_char = sum([criterion_char(restored[j],target) for j in range(len(restored))])\n",
    "        loss_edge = sum([criterion_edge(restored[j],target) for j in range(len(restored))])\n",
    "        loss = (loss_char) + (0.05*loss_edge)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss +=loss.item()\n",
    "\n",
    "    #### Evaluation ####\n",
    "    if epoch%opt.TRAINING.VAL_AFTER_EVERY == 0:\n",
    "        model_restoration.eval()\n",
    "        psnr_val_rgb = []\n",
    "        for ii, data_val in enumerate((val_loader), 0):\n",
    "            target = data_val[0].cuda()\n",
    "            input_ = data_val[1].cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                restored = model_restoration(input_)\n",
    "            restored = restored[0]\n",
    "\n",
    "            for res,tar in zip(restored,target):\n",
    "                psnr_val_rgb.append(utils.torchPSNR(res, tar))\n",
    "\n",
    "        psnr_val_rgb  = torch.stack(psnr_val_rgb).mean().item()\n",
    "\n",
    "        if psnr_val_rgb > best_psnr:\n",
    "            best_psnr = psnr_val_rgb\n",
    "            best_epoch = epoch\n",
    "            torch.save({'epoch': epoch, \n",
    "                        'state_dict': model_restoration.state_dict(),\n",
    "                        'optimizer' : optimizer.state_dict()\n",
    "                        }, os.path.join(model_dir,\"model_best.pth\"))\n",
    "\n",
    "        print(\"[epoch %d PSNR: %.4f --- best_epoch %d Best_PSNR %.4f]\" % (epoch, psnr_val_rgb, best_epoch, best_psnr))\n",
    "\n",
    "        torch.save({'epoch': epoch, \n",
    "                    'state_dict': model_restoration.state_dict(),\n",
    "                    'optimizer' : optimizer.state_dict()\n",
    "                    }, os.path.join(model_dir,f\"model_epoch_{epoch}.pth\")) \n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    print(\"Epoch: {}\\tTime: {:.4f}\\tLoss: {:.4f}\\tLearningRate {:.8f}\".format(epoch, time.time()-epoch_start_time, epoch_loss, scheduler.get_lr()[0]))\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "\n",
    "    torch.save({'epoch': epoch, \n",
    "                'state_dict': model_restoration.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict()\n",
    "                }, os.path.join(model_dir,\"model_latest.pth\")) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a091e28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T06:02:58.985574Z",
     "iopub.status.busy": "2025-12-21T06:02:58.985218Z",
     "iopub.status.idle": "2025-12-21T17:49:39.764392Z",
     "shell.execute_reply": "2025-12-21T17:49:39.763665Z"
    },
    "papermill": {
     "duration": 42400.783407,
     "end_time": "2025-12-21T17:49:39.765766",
     "exception": false,
     "start_time": "2025-12-21T06:02:58.982359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:1105: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\r\n",
      "  _warn_get_lr_called_within_step(self)\r\n",
      "------------------------------------------------------------------------------\r\n",
      "==> Resuming Training from epoch 81 with LR 0.00020000\r\n",
      "------------------------------------------------------------------------------\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n",
      "  warnings.warn(\r\n",
      "===> Start Epoch 81 End Epoch 101\r\n",
      "===> Loading datasets\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:11<00:00,  1.30it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 81\tTime: 2111.2725\tLoss: 183.2656\tLearningRate 0.00001935\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:14<00:00,  1.30it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 82\tTime: 2114.3209\tLoss: 183.0762\tLearningRate 0.00001753\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:15<00:00,  1.30it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 83\tTime: 2115.5245\tLoss: 182.9397\tLearningRate 0.00001580\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:15<00:00,  1.30it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 84\tTime: 2115.9910\tLoss: 182.6303\tLearningRate 0.00001416\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:15<00:00,  1.30it/s]\r\n",
      "[epoch 85 PSNR: 31.1289 --- best_epoch 85 Best_PSNR 31.1289]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 85\tTime: 2131.0822\tLoss: 182.3470\tLearningRate 0.00001261\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:15<00:00,  1.30it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 86\tTime: 2115.5636\tLoss: 182.1896\tLearningRate 0.00001115\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:15<00:00,  1.30it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 87\tTime: 2115.5633\tLoss: 182.1106\tLearningRate 0.00000978\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:16<00:00,  1.30it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 88\tTime: 2116.5402\tLoss: 182.2669\tLearningRate 0.00000851\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:17<00:00,  1.29it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 89\tTime: 2117.4746\tLoss: 181.9812\tLearningRate 0.00000734\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:16<00:00,  1.30it/s]\r\n",
      "[epoch 90 PSNR: 31.1422 --- best_epoch 90 Best_PSNR 31.1422]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 90\tTime: 2131.7817\tLoss: 181.8364\tLearningRate 0.00000626\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:16<00:00,  1.30it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 91\tTime: 2116.3858\tLoss: 181.7668\tLearningRate 0.00000528\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:16<00:00,  1.30it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 92\tTime: 2116.7247\tLoss: 181.5753\tLearningRate 0.00000441\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:16<00:00,  1.30it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 93\tTime: 2116.1561\tLoss: 181.5601\tLearningRate 0.00000363\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:16<00:00,  1.30it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 94\tTime: 2116.7240\tLoss: 181.3144\tLearningRate 0.00000295\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:17<00:00,  1.30it/s]\r\n",
      "[epoch 95 PSNR: 31.1484 --- best_epoch 95 Best_PSNR 31.1484]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 95\tTime: 2132.0319\tLoss: 181.4775\tLearningRate 0.00000238\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:17<00:00,  1.30it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 96\tTime: 2117.1133\tLoss: 181.2523\tLearningRate 0.00000190\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:17<00:00,  1.29it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 97\tTime: 2117.3875\tLoss: 181.1705\tLearningRate 0.00000153\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:17<00:00,  1.29it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 98\tTime: 2117.4018\tLoss: 181.3523\tLearningRate 0.00000126\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:17<00:00,  1.29it/s]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 99\tTime: 2117.4812\tLoss: 181.0209\tLearningRate 0.00000109\r\n",
      "------------------------------------------------------------------\r\n",
      "100%|███████████████████████████████████████| 2742/2742 [35:17<00:00,  1.29it/s]\r\n",
      "[epoch 100 PSNR: 31.1521 --- best_epoch 100 Best_PSNR 31.1521]\r\n",
      "------------------------------------------------------------------\r\n",
      "Epoch: 100\tTime: 2132.8355\tLoss: 181.0422\tLearningRate 0.00000101\r\n",
      "------------------------------------------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8656764,
     "sourceId": 13824812,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8984537,
     "sourceId": 14106088,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9033316,
     "sourceId": 14171807,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9062021,
     "sourceId": 14241566,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42415.046354,
   "end_time": "2025-12-21T17:49:42.478651",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-21T06:02:47.432297",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
